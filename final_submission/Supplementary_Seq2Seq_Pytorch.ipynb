{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplementary:\n",
    "## Absractive Seq2Seq GRU Model with Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/40982191/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import json\n",
    "import lzma\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords  \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview \n",
    "This supplementary notebook includes an implementation of the abastractive Seq2Seq model with attention using GRU. Different from the one included in the main notebook, this model is implemented in PyTorch, which is more often used for natural language processing and more efficient than Keras. This model uses encoders and attention decoders with two layers of GRU to genereate summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a fucnction to remove \\n and HTML tags\n",
    "# function adapted from https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text):\n",
    "    text_divided = text.splitlines()\n",
    "    text_divided_clean = \" \".join(text_divided)\n",
    "    text_divided_clean = text_divided_clean.lower()\n",
    "    text_divided_clean = re.sub('\"','', text_divided_clean) # remove '\"'\n",
    "    text_divided_clean = re.sub(r\"'s\\b\",\"\",text_divided_clean) # remove ''s'\n",
    "    text_divided_clean = re.sub(\"[^a-zA-Z]\", \" \", text_divided_clean) # removes all strings that contains a non-letter\n",
    "    return text_divided_clean\n",
    "\n",
    "# setting up tokenizer\n",
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(state):\n",
    "    # reading json files\n",
    "    cases = []\n",
    "    with lzma.open(state + '/data/data.jsonl.xz', 'r') as jsonl_file:\n",
    "        for case in jsonl_file:\n",
    "            cases.append(json.loads(str(case, 'utf-8')))\n",
    "\n",
    "    df = pd.DataFrame(cases).sort_values('decision_date').reset_index(drop=True)\n",
    "    df['decision_date'] = pd.to_datetime(df['decision_date'])\n",
    "\n",
    "    # parsing data\n",
    "    storage = []\n",
    "    for i in range(df.shape[0]):\n",
    "        casebody_idx = df.columns.get_loc(\"casebody\")\n",
    "        judges = df.iloc[i,casebody_idx]['data']['judges']\n",
    "        attorneys = df.iloc[i,casebody_idx]['data']['attorneys']\n",
    "        headnotes = df.iloc[i,casebody_idx]['data']['head_matter']\n",
    "        if df.iloc[i,casebody_idx]['data']['opinions'] != []:\n",
    "            opinions = df.iloc[i,casebody_idx]['data']['opinions'][0]['text']\n",
    "\n",
    "        headnotes_clean = text_cleaner(headnotes)\n",
    "        opinions_clean = text_cleaner(opinions)\n",
    "\n",
    "        storage.append({'judges': judges,\n",
    "                        'attorneys': attorneys,\n",
    "                        'headnote': headnotes_clean,\n",
    "                        'opinion_text': opinions_clean})\n",
    "    df_parsed = pd.DataFrame(storage)\n",
    "    df = df_parsed.merge(df, left_index=True, right_index=True)\n",
    "\n",
    "    # tokenizing headnotes and opinions\n",
    "    df['headnotes_token'] = df['headnote'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    df['opinions_token'] = df['opinion_text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    df['headnotes_num_tokens'] = [len(notes) for notes in df['headnotes_token']]\n",
    "    df['opinions_num_tokens'] = [len(opinions) for opinions in df['opinions_token']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get North Carolina data \n",
    "df_nc = get_data('North Carolina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation \n",
    "The implementation of a Seq2Seq model with GRU below is adapted from https://www.kaggle.com/rahuldshetty/text-summarization-in-pytorch with the following changes to accomodate the North Carolina dataset: \n",
    "1. Adapt the evaluate function to test trained model on validation data;\n",
    "2. Adjust the max length requirement on the sentence level;\n",
    "3. Add one more layer of GRU in encoder, decoder, and attention-decoder; \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation below is borrowed from \n",
    "# https://www.kaggle.com/rahuldshetty/text-summarization-in-pytorch\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "MAX_LENGTH = 1000 # Max length of the summary \n",
    "teacher_forcing_ratio = 0.5 # percentage of training to use teacher forcing \n",
    "\n",
    "# language class to store words and embeddings \n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # beginning and end of sentence \n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# read into the language class \n",
    "def readLangs(text, summary, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(summary)\n",
    "        output_lang = Lang(text)\n",
    "    else:\n",
    "        input_lang = Lang(text)\n",
    "        output_lang = Lang(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# read all texts and summaries into the language class \n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# define an RNN encoder class with 2 layers of GRU \n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)# get embeddings \n",
    "        output = embedded\n",
    "        # two layers of GRU \n",
    "        output1, hidden1 = self.gru(output, hidden)\n",
    "        output, hidden = self.gru(output1, hidden1)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# RNN decoder class with 2 layers of GRU \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output1, hidden1 = self.gru(output, hidden)\n",
    "        output, hidden = self.gru(output1, hidden1)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "# decoder with attention \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1) # get embeddings \n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # update weights for attention scores \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        # concatenate attnetion scores to output\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        # two layers of GRU\n",
    "        output1, hidden1 = self.gru(output, hidden)\n",
    "        output, hidden = self.gru(output1, hidden1)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "# prepare words from sentence     \n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "# create tensor from words of each sentence\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "# create tensors from all texts and summaries \n",
    "def tensorsFromPair(input_lang,output_lang,pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "# a function to train encoder and decoder \n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    # initialize optimizer \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0 # initialize loss \n",
    "    \n",
    "    # decide proper input length \n",
    "    if input_length > max_length:\n",
    "        length = max_length\n",
    "    else:\n",
    "        length = input_length\n",
    "        \n",
    "    # run encoder on each tensor \n",
    "    for ei in range(length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # decide if use teacher forcing to improve efficiency \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # get hidden state with attention scores \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs) # use actual target as input \n",
    "            # update loss function \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  \n",
    "\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            # get hidden state with attention scores \n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)# get the most recent prediction \n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    # update optimizer \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "\n",
    "# train iteratively \n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
    "    print(\"Training....\")\n",
    "    print_loss_total = 0  \n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    # generate tranining pairs from the number of iterations \n",
    "    training_pairs = [tensorsFromPair(x_train_lang,y_train_lang,random.choice(train_pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # iterate through \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1)\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        # apply train function in each iteration and updates loss \n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(%d %d%%) %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "def evaluate(encoder, decoder, in_lang,out_lang,sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # get tensor from input \n",
    "        input_tensor = tensorFromSentence(in_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        # initialize output \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        \n",
    "        # check for max length requirement \n",
    "        if input_length > max_length:\n",
    "            length = max_length\n",
    "        else:\n",
    "            length = input_length\n",
    "        \n",
    "        # generate each token from encoder \n",
    "        for ei in range(length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "    \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = [] # initialize decoded words \n",
    "        decoder_attentions = torch.zeros(max_length, max_length) # initialize attention \n",
    "        # generate each token from attention decoder \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs) # get decoder output \n",
    "            decoder_attentions[di] = decoder_attention.data # get attention scores \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token: \n",
    "                decoded_words.append('<EOS>') # break when reaching end of sentence \n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(out_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "train_size = 5000\n",
    "val_size = 100\n",
    "\n",
    "train_df = df_nc.sample(n=train_size,replace=False, random_state=1)\n",
    "val_df = df_nc.sample(n=val_size,replace=False, random_state=1)\n",
    "\n",
    "x_train,y_train = train_df.opinion_text.tolist(),train_df.headnote.tolist()\n",
    "x_val,y_val = val_df.opinion_text.tolist(),val_df.headnote.tolist()\n",
    "\n",
    "x_train_lang, y_train_lang, train_pairs = prepareData( x_train, y_train , False)\n",
    "x_val_lang, y_val_lang, val_pairs = prepareData( x_val, y_val , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "(1 1%) 10.4913\n",
      "(2 2%) 9.7428\n",
      "(3 3%) 41.4239\n",
      "(4 4%) 167.3436\n",
      "(5 5%) 51.5854\n",
      "(6 6%) 40.2086\n",
      "(7 7%) 38.4778\n",
      "(8 8%) 118.7396\n",
      "(9 9%) 130.1008\n",
      "(10 10%) 594.8017\n",
      "(11 11%) 336.5368\n",
      "(12 12%) 788.0958\n",
      "(13 13%) 269.5146\n",
      "(14 14%) 117.3960\n",
      "(15 15%) 298.8359\n",
      "(16 16%) 42.6356\n",
      "(17 17%) 157.3596\n",
      "(18 18%) 963.7615\n",
      "(19 19%) 812.1887\n",
      "(20 20%) 907.2435\n",
      "(21 21%) 1541.8153\n",
      "(22 22%) 827.2287\n",
      "(23 23%) 1141.2617\n",
      "(24 24%) 715.3318\n",
      "(25 25%) 599.7248\n",
      "(26 26%) 1374.8220\n",
      "(27 27%) 521.2188\n",
      "(28 28%) 541.5331\n",
      "(29 28%) 494.0919\n",
      "(30 30%) 771.2525\n",
      "(31 31%) 479.9329\n",
      "(32 32%) 476.3551\n",
      "(33 33%) 379.4948\n",
      "(34 34%) 278.6584\n",
      "(35 35%) 237.9760\n",
      "(36 36%) 305.7043\n",
      "(37 37%) 104.3085\n",
      "(38 38%) 59.4516\n",
      "(39 39%) 46.9063\n",
      "(40 40%) 198.6581\n",
      "(41 41%) 279.5665\n",
      "(42 42%) 344.4051\n",
      "(43 43%) 306.1651\n",
      "(44 44%) 125.0189\n",
      "(45 45%) 104.2100\n",
      "(46 46%) 162.1568\n",
      "(47 47%) 112.2507\n",
      "(48 48%) 101.6270\n",
      "(49 49%) 217.0972\n",
      "(50 50%) 67.2271\n",
      "(51 51%) 250.2664\n",
      "(52 52%) 298.4876\n",
      "(53 53%) 212.7325\n",
      "(54 54%) 32.1909\n",
      "(55 55%) 247.3997\n",
      "(56 56%) 438.2434\n",
      "(57 56%) 304.2232\n",
      "(58 57%) 299.7157\n",
      "(59 59%) 295.6115\n",
      "(60 60%) 221.7519\n",
      "(61 61%) 58.2953\n",
      "(62 62%) 155.7128\n",
      "(63 63%) 1.9610\n",
      "(64 64%) 196.5567\n",
      "(65 65%) 208.0424\n",
      "(66 66%) 367.4740\n",
      "(67 67%) 271.2635\n",
      "(68 68%) 96.5441\n",
      "(69 69%) 131.4365\n",
      "(70 70%) 468.4212\n",
      "(71 71%) 517.0010\n",
      "(72 72%) 255.3827\n",
      "(73 73%) 179.5062\n",
      "(74 74%) 410.0184\n",
      "(75 75%) 350.6626\n",
      "(76 76%) 263.7782\n",
      "(77 77%) 207.3674\n",
      "(78 78%) 219.5384\n",
      "(79 79%) 213.0492\n",
      "(80 80%) 269.2479\n",
      "(81 81%) 177.0556\n",
      "(82 82%) 365.1893\n",
      "(83 83%) 239.4511\n",
      "(84 84%) 273.9896\n",
      "(85 85%) 233.6221\n",
      "(86 86%) 263.5217\n",
      "(87 87%) 172.9921\n",
      "(88 88%) 195.9233\n",
      "(89 89%) 368.7303\n",
      "(90 90%) 630.9615\n",
      "(91 91%) 316.2917\n",
      "(92 92%) 247.6873\n",
      "(93 93%) 215.9925\n",
      "(94 94%) 178.4449\n",
      "(95 95%) 204.4813\n",
      "(96 96%) 1179.3019\n",
      "(97 97%) 481.9203\n",
      "(98 98%) 573.1274\n",
      "(99 99%) 444.7789\n",
      "(100 100%) 378.5494\n"
     ]
    }
   ],
   "source": [
    "# see original training iteration scope at\n",
    "# https://www.kaggle.com/rahuldshetty/text-summarization-in-pytorch\n",
    "hidden_size = 150\n",
    "encoder1 = EncoderRNN(x_train_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, y_train_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 100, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights of the trained model \n",
    "torch.save(encoder1.state_dict(), './supp_seq2seq_encoder.w')\n",
    "torch.save(attn_decoder1.state_dict(), './supp_seq2seq_attention_decoder.w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate and report ROUGE scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max recurssion for ROUGE \n",
    "#https://github.com/pltrdy/rouge/issues/19\n",
    "sys.setrecursionlimit(train_size * MAX_LENGTH + 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge #https://pypi.org/project/rouge/\n",
    "rouge = rouge.Rouge()\n",
    "# test on 100 validation samples \n",
    "scores_r1 = np.zeros(val_size) # ROUGE-1\n",
    "scores_r2 = np.zeros(val_size) # ROUGE-2\n",
    "for i in range(val_size):\n",
    "    pair = val_pairs[i]\n",
    "    # get generated words \n",
    "    out_words, _ = evaluate(encoder1, attn_decoder1, x_val_lang,y_val_lang, pair[0])\n",
    "    # concatenate the genrated words into full summary \n",
    "    out_sentence = ' '.join(out_words)\n",
    "    sc = rouge.get_scores(out_sentence, pair[1])\n",
    "    scores_r1[i] = sc[0]['rouge-1']['f']\n",
    "    scores_r2[i] = sc[0]['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average Rouge 1 F-scores on 1000 random cases is\n",
      "0.007588122918098881\n",
      "The average Rouge 2 F-scores on 1000 random cases is\n",
      "5.2490542906115484e-05\n"
     ]
    }
   ],
   "source": [
    "print('The average Rouge 1 F-scores on 1000 random cases is')\n",
    "print(np.mean(scores_r1))\n",
    "print('The average Rouge 2 F-scores on 1000 random cases is')\n",
    "print(np.mean(scores_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "We decided against including this model due to its poor performance and its similarity to the Abastract Seq2Seq model in the main notebook. Nevertheless, we believe this model can further prove that abstractive models are not suitable with legal texts, potentially due to the thematic structures and citations of the legal texts, which are discussed in the main notebook. At the same time, the model also requires deep training. As seen in the original code, the model was intended to train for iterations in the hundred-thousand scale, which is not achieveable given our computing resources. Therefore, we cannot rule out the possibility that with deeper training, this model might perform well. Still, extractive models are more sensible choices give the scope of this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
